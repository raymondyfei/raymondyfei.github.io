<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<HTML lang=en xmllang="en" xmlns="http://www.w3.org/1999/xhtml">

  <HEAD>
    <TITLE>Raymond Yun Fei | Adobe</TITLE>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <META content="text/html; charset=utf-8" http-equiv=Content-Type>
    <meta name="keywords" content="cloth computation drag effect equations fabric volume solve surface flow fluid forces hair interaction 
            voxel simulation liquid method model wet porous water solid velocity">
    <script>
      <!--
      function getStyleSheet() {
        var currentTime = new Date().getHours();
        if ((0 <= currentTime && currentTime < 6) || (18 <= currentTime && currentTime <= 24)) {
          document.write("<LINK rel=stylesheet type=text/css href='night.css' media=screen>");
        } else {
          document.write("<LINK rel=stylesheet type=text/css href='day.css' media=screen>");
        }
      }
      getStyleSheet();
      -->
    </script>
    <noscript>
      <LINK rel=stylesheet type=text/css href="day.css" media=screen>
    </noscript>
    <link rel="stylesheet" type="text/css" href="dist/lovely-tag.css">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css">
    <link href="fontawesome/css/all.min.css" rel="stylesheet">
    <link rel="icon" href="icon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href='https://fonts.googleapis.com/css2?family=Oswald:wght@600;700&family=Titillium+Web:wght@300;600&display=swap' rel='stylesheet'>
    <META name=GENERATOR content="MSHTML 8.00.7601.17785">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116473534-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());

      gtag('config', 'UA-116473534-1');
    </script>
  </HEAD>

  <BODY>
    <DIV id=wrapper>
      <DIV id=logo>
        <IMG SRC="headshot.jpg" alt="headshot" WIDTH="100%" />
        <DIV id=introduction class=intro>
          <P>
            <b>RAYMOND YUN FEI<br />
              PH. D.</b><br /><br />
            SENIOR RESEARCH ENGINEER<br />
            3D & AI GRAPHICS<br />
            <a href="https://www.adobe.com/creativecloud/3d-ar.html">ADOBE</a> - <a href="https://substance3devents.com/">SUBSTANCE 3D</a><br />
            <b><a href="https://substance3devents.com/substance-careers">WE ARE HIRING!</a></b>
          </P>
          <P style="font-size: 110%">
            <a href="#head-email" class="open-popup-link"><i class="fas fa-envelope"></i></a>
            <a target="_blank" href="https://github.com/raymondyfei"><i class="fab fa-github"></i></a>
            <a target="_blank" href="https://www.linkedin.com/in/raymond-fei-8169b059/"><i class="fab fa-linkedin"></i></a>
            <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=Zyh1OcMAAAAJ"><i class="fab fa-google"></i></a>
            <div id="head-email" class="white-popup mfp-hide">
              <p><IMG SRC="sendo.jpg" alt="info" WIDTH="100%" /></p>
            </div>
          </P>
          <p style="letter-spacing: 0.2px">
            Raymond Yun Fei connects research with <a href="https://www.adobe.com/creativecloud/3d-ar.html">Adobe 3D & AR</a>
            products through <a href="https://en.wikipedia.org/wiki/Technology_transfer">tech
              transfers</a> in physics, geometry, rendering, deep learning, and GPU computing.
            Raymond published
            research works of a broad
            spectrum in <a href="https://en.wikipedia.org/wiki/Computer_graphics_(computer_science)#:~:text=Computer%20graphics%20is%20a%20sub,dimensional%20graphics%20and%20image%20processing.">computer
              graphics</a>, which have been covered by multiple well-known media. Raymond has
            also served as a reviewer for
            top
            academic
            conferences such as <a href="https://www.siggraph.org/conference-and-events/annual-conferences/">SIGGRAPH
              North America, SIGGRAPH Asia</a> and <a href="https://nips.cc/">NeurIPS</a>, and journals such as <a href="https://dl.acm.org/journal/tog">ACM Transactions on Graphics (TOG)</a> and <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=2945">IEEE Transactions on Visualization and
              Computer Graphics (TVCG)</a>.
          </p>
        </DIV>
        <DIV id=other class=detail>
          <div class="image-holder">
            <p><a href="https://pcs-sim.github.io/"><img src="banners/pcs.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="gpu_mpm/"><img src="banners/crawling_banner.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="asflip/"><img src="banners/asflip_banner.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="https://ge.in.tum.de/publications/2020-um-solver-in-the-loop/"><img src="banners/sitl.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="http://www.cs.columbia.edu/cg/creamystrand/"><img src="banners/pasta_banner.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="https://arxiv.org/abs/1904.11116"><img src="banners/glove.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="http://www.cs.columbia.edu/cg/wetcloth/"><img src="banners/twist.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="http://www.cs.columbia.edu/cg/liquidhair/"><img src="banners/dog_teasor_10x3.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="http://www.cs.columbia.edu/cg/transfer/"><img src="banners/iat.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="http://people.seas.harvard.edu/~gaurav/papers/cdmcs_sa_2015/"><img src="banners/cdmcs_sa_2015_teaser.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="http://www.cs.columbia.edu/~fyun/watercolor/watercolor_pp.pdf"><img src="banners/watercolor.jpg" alt="gallery" width="100%"></a></p>
            <p><a href="http://www.liyiwei.org/papers/noise-siga13/"><img src="banners/sea.jpg" alt="gallery" width="100%"></a></p>
          </div>
        </DIV>
      </DIV>
      <DIV id=content>
        <DIV class=x></DIV>
        <DIV id=maincontent>
          <DIV id=techtransfers class=detail>
            <P class=staffitem>TECH TRANSFERS</P>
            <p>
              <b>Rasterization and GPU Pathtracing of Gaussian Splats</b><br />
              Shipped in <a href="https://helpx.adobe.com/substance-3d-viewer.html">Adobe Substance 3D Viewer</a> (Beta), 
2024<br />
              <a 
href="https://www.linkedin.com/posts/martin-nebelong_adobe-art-gaussiansplatting-activity-7252067862486405120-vNgA/">LinkedIn Post</a> | In Press: <a href="https://www.creativebloq.com/3d/adobes-new-3d-tool-is-more-powerful-than-you-might-think">Creative Bloq</a><br 
/>
              In collaboration with other team members in 3D&I and Research.<br/>
              <span class="tag tag-purple">GPU</span> <span class="tag tag-green2">machine learning</span> <span class="tag tag-blue2">rendering</span>
            </p>
            <p>
              <b>AI-based Search in 3D Asset Library</b><br />
              Shipped in <a href="https://www.adobe.com/products/substance3d-stager.html">Adobe Substance Modeler</a> 1.8.50 (Public Beta), 
2024<br />
              <a href="https://helpx.adobe.com/substance-3d-modeler/release-notes/public-beta/beta-1-8-50-release-notes.html">Official Release Note</a> | <a 
href="https://www.linkedin.com/posts/lydiachoy_here-is-a-better-example-of-search-by-shape-activity-7175684982630232065-Cc8Y/">LinkedIn Post</a><br 
/>
              In collaboration with other team members in 3D&I and Research.<br/>
              <span class="tag tag-green2">machine learning</span> <span class="tag tag-orange">geometry</span>
            </p>
            <p>
              <b>Fast GPU AI Denoiser</b><br />
              Shipped in <a href="https://www.adobe.com/products/substance3d-stager.html">Adobe Substance Stager</a> 2.1 (Offline), 3.0 (Interactive), and <a href="https://helpx.adobe.com/substance-3d-viewer.html">Adobe Substance 3D Viewer</a> (Beta), 2023-2024<br/>
              Official Release Note: <a href="https://helpx.adobe.com/substance-3d-stager/release-notes/version-2-1-0.html">Stager 2.1</a>, 
              <a href="https://helpx.adobe.com/substance-3d-stager/release-notes/version-3-0.html">Stager 3.0</a>,
              <a href="https://helpx.adobe.com/substance-3d-viewer/interface/environment-workspace.html">3D Viewer</a> |
              <a href="https://www.linkedin.com/posts/substance3d_substance3dstager-activity-7077646473198276609-bYIs/">Official LinkedIn Post</a> | 
              <a href="https://blog.adobe.com/en/publish/2023/07/27/substance-3d-collection-july-release-news">Benchmarks</a> |
              In Press: <a href="https://80.lv/articles/adobe-s-substance-3d-stager-3-0-generates-backgrounds-from-prompts/">80.lv</a>, 
              <a href="https://www.cgchannel.com/2024/03/adobe-releases-substance-3d-stager-3-0/">CG Channel</a><br/>
              In collaboration with other team members in 3D&I and Research.<br/>
              <span class="tag tag-purple">GPU</span> <span class="tag tag-green2">machine learning</span> <span class="tag tag-blue2">rendering</span>
            </p>
            <p>
              <b>Wet Hairs Simulation</b><br />
              Shipped in <a href="http://alexey.stomakhin.com/research/siggraph2022_loki.pdf">Wētā FX Loki</a>, used in productions such as <i>Alita: Battle Angel</i>, 
2017<br />
              <a href="https://www.imdb.com/name/nm10481123/">Film Credit</a><br />
              In collaboration with other team members in the simulation department.<br/>
              <span class="tag tag-blue">simulation</span>
            </p>
          </DIV>
          <DIV id=pubs class=detail>
            <P class=staffitem>PUBLICATIONS</P>
            <P>
              <b>Trading Spaces: Adaptive Subspace Time Integration for Contacting Elastodynamics</b><br />
              ACM Transactions on Graphics (SIGGRAPH Asia), 2024<br />
              <span class=name>
                <a href="https://www.dgp.toronto.edu/~trusty/">Ty Trusty</a>,
                <STRONG>Yun (Raymond) Fei</STRONG>,
                <a href="http://diwlevin.com/">David I. W. Levin</a>,
                <a href="https://dannykaufman.io/">Danny M. Kaufman</a>
                </span>
              <br />
              <a href="#abs-tsa" class="open-popup-link">Abstract</a> | <a href="https://www.dgp.toronto.edu/projects/trading-spaces/">Project Page</a><br />
              <span class="tag tag-blue">simulation</span> <span class="tag tag-orange">geometry</span>
            </p>
            <div id="abs-tsa" class="white-popup mfp-hide">
              <p>
                We construct a subspace simulator that adaptively balances solution improvement against system size. The core components of our simulator are an adaptive subspace oracle, model, and parallel time-step solver algorithm. Our in-time-step adaptivity oracle continually assesses subspace solution quality and candidate update proposals while accounting for temporal variations in deformation and spatial variations in material. In turn our adaptivity model is subspace agnostic. It allows application across subspace representations and expresses unrestricted deformations independent of subspace choice. We couple our oracle and model with a custom-constructed parallel time-step solver for our enriched systems that exposes a pair of user tolerances which provide controllable simulation quality. As tolerances are tightened our model converges to full-space solutions (with expected cost increases). On the other hand, as tolerances are relaxed we obtain output-bound simulation costs. We demonstrate the efficacy of our approach across a wide range of challenging nonlinear materials models, material stiffnesses, heterogeneities, dynamic behaviors, and frictionally contacting conditions, obtaining scalable and efficient simulations of complex elastodynamic scenarios.
              </p>
            </div>
            <P>
              <b>Progressive Shell Quasistatics for Unstructured Meshes</b><br />
              ACM Transactions on Graphics (SIGGRAPH Asia), 2023
              <br />
              <span class=name>
                            <a href="https://eriszhang.github.io/">Jiayi Eris Zhang</a>,
                            <a href="https://www.jdumas.org/">Jérémie Dumas</a>,
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>,
                            <a href="https://graphics.stanford.edu/~djames/">Doug L. James</a>,
                            <a href="https://dannykaufman.io/">Danny M. Kaufman</a>
                            </span>
              <br />
              <a href="#abs-psq" class="open-popup-link">Abstract</a> | <a href="https://pcs-sim.github.io/psq/">Project Page</a><br />
              <span class="tag tag-blue">simulation</span> <span class="tag tag-orange">geometry</span>
            </p>
            <div id="abs-psq" class="white-popup mfp-hide">
              <p>
                Thin shell structures exhibit complex behaviors critical for modeling and design across wide-ranging applications. Capturing their 
    mechanical response requires finely detailed, high-resolution meshes. Corresponding simulations for predicting equilibria with these meshes are 
    expensive, whereas coarse-mesh simulations can be fast but generate unacceptable artifacts and inaccuracies. The recently proposed progressive 
    simulation framework [Zhang et al. 2022] offers a promising avenue to address these limitations with consistent and progressively improving simulation 
    over a hierarchy of increasingly higher-resolution models. Unfortunately, it is currently severely limited in application to meshes and shapes generated 
    via Loop subdivision.
        </p>
        <p>
                We propose Progressive Shells Quasistatics to extend progressive simulation to the high-fidelity modeling and design of all input shell (and plate) 
                geometries with unstructured (as well as structured) triangle meshes. To do so, we construct a fine-to-coarse hierarchy with a novel nonlinear 
                prolongation operator custom-suited for curved-surface simulation that is rest-shape preserving, supports complex curved boundaries, and enables the 
                reconstruction of detailed geometries from coarse-level meshes. Then, to enable convergent, high-quality solutions with robust contact handling, we 
                propose a new, safe, and efficient shape-preserving upsampling method that ensures non-intersection and strain limits during refinement. With these core 
                contributions, Progressive Shell Quasistatics enables, for the first time, wide generality for progressive simulation, including support for arbitrary 
                curved-shell geometries, progressive collision objects, curved boundaries, and unstructured triangle meshes – all while ensuring that preview and final 
                solutions remain free of intersections. We demonstrate these features across a wide range of stress tests where progressive simulation captures the 
                wrinkling, folding, twisting, and buckling behaviors of frictionally contacting thin shells with orders-of-magnitude speed-up in examples over direct 
                fine-resolution simulation.
              </p>
            </div>
            <P>
              <b>Progressive Simulation for Cloth Quasistatics</b><br />
              ACM Transactions on Graphics (SIGGRAPH Asia), 2022, appeared on the <a href="pcs/3550454.fm.pdf">back cover</a> 
              <i class="fas fa-award"></i><br />
              <span class=name>
                            <a href="https://eriszhang.github.io/">Jiayi Eris Zhang</a>,
                            <a href="https://www.jdumas.org/">Jérémie Dumas</a>,
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a>,
                            <a href="https://graphics.stanford.edu/~djames/">Doug L. James</a>,
                            <a href="https://dannykaufman.io/">Danny M. Kaufman</a>
                            </span>
              <br />
              <a href="#abs-pcs" class="open-popup-link">Abstract</a> | <a href="https://pcs-sim.github.io/">Project Page</a><br />
              <span class="tag tag-blue">simulation</span> <span class="tag tag-orange">geometry</span>
            </p>
            <div id="abs-pcs" class="white-popup mfp-hide">
              <p>
                The trade-off between speed and fidelity in cloth simulation is a fundamental computational problem in computer
                graphics and computational design. Coarse cloth models provide the interactive performance required by designers, but they can not be simulated at
                higher resolutions (“up-resed”) without introducing simulation artifacts and/or unpredicted outcomes, such as different folds, wrinkles and drapes. But
                how can a coarse simulation predict the result of an unconstrained, high-resolution simulation that has not yet been run? <br /><br />
                We propose Progressive Cloth Simulation (PCS), a new forward simulation method for efficient preview of cloth quasistatics on exceedingly coarse
                triangle meshes with consistent and progressive improvement over a hierarchy of increasingly higher-resolution models. PCS provides an efficient coarse
                previewing simulation method that predicts the coarse-scale folds and wrinkles that will be generated by a corresponding converged, high-fidelity C-IPC
                simulation of the cloth drape’s equilibrium. For each preview PCS can generate an increasing-resolution sequence of consistent models that progress
                towards this converged solution. This successive improvement can then be interrupted at any point, for example, whenever design parameters are updated.
                PCS then ensures feasibility at all resolutions, so that predicted solutions remain intersection-free and capture the complex folding and buckling
                behaviors of frictionally contacting cloth.
              </p>
            </div>
            <P>
              <b>Principles towards Real-Time Simulation of Material Point Method on Modern GPUs</b><br />
              Talks on Game Developers Conference (GDC) and NVIDIA GPU Technology Conference (GTC), 2022<br />
              <span class=name>
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://www.linkedin.com/in/yuhan-h-19024a18b/">Yuhan Huang</a>,
                            <a href="https://mingg13.github.io/">Ming Gao</a>
                            </span>
              <br />
              <a href="#abs-prtsm" class="open-popup-link">Abstract</a> | <a href="gpu_mpm/">Project Page</a><br />
              <span class="tag tag-purple">GPU</span> <span class="tag tag-blue">simulation</span>
            </p>
            <div id="abs-prtsm" class="white-popup mfp-hide">
              <p>Physics-based simulation has been actively employed in generating offline visual effects in the film and animation industry. However, the computations required for high-quality scenarios are generally
                immense, deterring its adoption in real-time applications, e.g., virtual production, avatar live-streaming, and cloud gaming. We summarize the principles that can accelerate the computation pipeline on single-GPU and multi-GPU
                platforms through extensive investigation and comprehension of modern GPU architecture. We further demonstrate the effectiveness of these principles by applying them to the material point method to build up our framework, which
                achieves 1.7×--8.6× speedup on a single GPU and 2.5×--14.8× on four GPUs compared to the state-of-the-art. Our pipeline is specifically designed for real-time applications (i.e., scenarios with small to medium particles) and achieves
                significant multi-GPU efficiency. We demonstrate our pipeline by simulating a snow scenario with 1.33M particles and a fountain scenario with 143K particles in real-time (on average, 68.5 and 55.9 frame-per-second, respectively) on
                four NVIDIA Tesla V100 GPUs interconnected with NVLinks.
              </p>
            </div>
            <div id="bib-prtsm" class="white-popup mfp-hide">
              <p>
                @article{fei2021principles,<br />
                &nbsp;&nbsp;author = {Fei, Yun (Raymond) and Huang, Yuhan and Gao, Ming},<br />
                &nbsp;&nbsp;title = {Principles towards Real-Time Simulation of Material Point Method on Modern GPUs},<br />
                &nbsp;&nbsp;journal = {arXiv preprint},<br />
                &nbsp;&nbsp;volume = {abs/2111.00699},<br />
                &nbsp;&nbsp;year = {2021},<br />
                &nbsp;&nbsp;url = {https://arxiv.org/abs/2111.00699},<br />
                &nbsp;&nbsp;eprinttype = {arXiv},<br />
                &nbsp;&nbsp;eprint = {2111.00699}<br />
                }
              </p>
            </div>
            <P>
              <b>Revisiting Integration in the Material Point Method: A Scheme for Easier Separation and Less Dissipation</b><br />
              ACM Transactions on Graphics (SIGGRAPH North America), 2021<br />
              <span class=name>
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://qig.github.io/">Qi Guo</a>,
                            <a href="https://www.linkedin.com/in/rundongwu/">Rundong Wu</a>,
                            <a href="https://www.linkedin.com/in/huangli1037/">Li Huang</a>, 
                            <a href="https://mingg13.github.io/">Ming Gao</a>
                            </span>
              <br />
              <a href="#abs-rimpm" class="open-popup-link">Abstract</a> | <a href="asflip/">Project Page</a> | In Press: <a href="https://youtu.be/Lp4k4O_HEeQ">Two Minute Papers</a>, <a href="https://www.gameres.com/884567.html">GameRes (in Chinese)</a>, <a href="https://techacute.com/graphics-material-point-method-video/">TechAcute</a><br />
              <span class="tag tag-blue">simulation</span>
            </p>
            <div id="abs-rimpm" class="white-popup mfp-hide">
              <p>
                The material point method recently demonstrated its efficacy at simulating many materials and the coupling between them on a massive scale. However, in scenarios containing debris, MPM manifests more dissipation and numerical viscosity than traditional Lagrangian methods. We have two observations from carefully revisiting existing integration methods used in MPM. First, nearby particles would end up with smoothed velocities without recovering momentum for each particle during the particle-grid-particle transfers. Second, most existing integrators assume continuity in the entire domain and advect particles by directly interpolating the positions from deformed nodal positions, which would trap the particles and make them harder to separate. We propose an integration scheme that corrects particle positions at each time step. We demonstrate our method's effectiveness with several large-scale simulations involving brittle materials. Our approach effectively reduces diffusion and unphysical viscosity compared to traditional integrators.
              </p>
            </div>
            <P>
              <b>Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers</b><br />
              Proceedings of NeurIPS, 2020<br />
              <span class=name>
                            <a href="https://perso.telecom-paristech.fr/kum/">Kiwon Um</a>,
                            <a href="https://robul.lavch.de/">Robert Brand</a>,
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://scholar.google.com/citations?user=LilimmEAAAAJ&hl=en">Philipp Holl</a>,
                            <a href="https://ge.in.tum.de/about/n-thuerey/">Nils Thuerey</a>
                            </span>
              <br />
              <a href="#abs-sitl" class="open-popup-link">Abstract</a> | <a href="https://ge.in.tum.de/publications/2020-um-solver-in-the-loop/">Project Page</a> | <a href="https://arxiv.org/abs/2007.00016">arXiv</a> | <a href="https://github.com/tum-pbs/Solver-in-the-Loop">GitHub</a> | <a href="https://www.youtube.com/watch?v=BwuRTpTR2Rg">YouTube</a><br />
              <span class="tag tag-blue">simulation</span> <span class="tag tag-green2">machine learning</span>
            </p>
            <div id="abs-sitl" class="white-popup mfp-hide">
              <p>
                Finding accurate solutions to partial differential equations (PDEs) is a crucial task in all scientific and engineering disciplines. It has recently been shown that machine learning methods can improve the solution accuracy by correcting for effects not captured by the discretized PDE. We target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. We find that previously used learning approaches are significantly outperformed by methods that integrate the solver into the training loop and thereby allow the model to interact with the PDE during training. This provides the model with realistic input distributions that take previous corrections into account, yielding improvements in accuracy with stable rollouts of several hundred recurrent evaluation steps and surpassing even tailored supervised variants. We highlight the performance of the differentiable physics networks for a wide variety of PDEs, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows.
              </p>
            </div>
            <P>
              <b>Multi-Scale Models to Simulate Interactions between Liquid and Thin Structures</b><br />
              Dissertation at Columbia University, honorable mention for <a href="https://www.siggraph.org/about/awards/outstanding-doctoral-dissertation-award/">Outstanding SIGGRAPH
                Dissertation Award</a>, 2020 <i class="fas fa-award"></i><br />
              <span class=name>
                            Advisors: <a href="https://www.dgp.toronto.edu/~eitan/">Eitan Grinspun</a>,
                            <a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>
                            </span>
              <br />
              <a href="#abs-msmsilts" class="open-popup-link">Abstract</a> | <a href="thesis/PhD_Dissertation_reduced.pdf">Preprint (Compressed)</a> | <a href="https://academiccommons.columbia.edu/doi/10.7916/d8-gyd1-rf76">Columbia Academic Commons</a><br />
              <span class="tag tag-blue">simulation</span>
            </P>
            <div id="abs-msmsilts" class="white-popup mfp-hide">
              <p>
                In this dissertation, we introduce a framework for simulating the dynamics between liquid and thin structures, including the effects of buoyancy, drag, capillary cohesion, dripping, and diffusion. After introducing related works, <b>Part I</b> begins with a discussion on the interactions between Newtonian fluid and fabrics. In this discussion, we treat both the fluid and the fabrics as continuum media; thus, the physical model is built from <i>mixture theory</i>. In <b>Part II</b>, we discuss the interactions between Newtonian fluid and hairs. To have more detailed dynamics, we no longer treat the hairs as continuum media. Instead, we treat them as discrete Kirchhoff rods. To deal with the thin layer of liquid that clings to the hairs, we augment each hair strand with a height field representation, through which we introduce a new reduced-dimensional flow model to solve the motion of liquid along the longitudinal direction of each hair. In addition, we develop a faithful model for the hairs' cohesion induced by surface tension, where a penalty force is applied to simulate the collision and cohesion between hairs. To enable the discrete strands interact with continuum-based, shear-dependent liquid, in <b>Part III</b>, we develop models that account for the volume change of the liquid as it passes through strands and the momentum exchange between the strands and the liquid. Accordingly, we extend the reduced-dimensional flow model to simulate liquid with elastoviscoplastic behavior. Furthermore, we use a constraint-based model to replace the penalty-force model to handle contact, which enables an accurate simulation of the frictional and adhesive effects between wet strands. We also present a principled method to preserve the total momentum of a strand and its surface flow, as well as an analytic plastic flow approach for Herschel-Bulkley fluid that enables stable semi-implicit integration at larger time steps.
                <br />
                We demonstrate a wide range of effects, including the challenging animation scenarios involving splashing, wringing, and colliding of wet clothes, as well as flipping of hair, animals shaking, spinning roller brushes from car washes being dunked in water, and intricate hair coalescence effects. For complex liquids, we explore a series of challenging scenarios, including strands interacting with oil paint, mud, cream, melted chocolate, and pasta sauce.
              </p>
            </div>
            <P>
              <b>A Multi-Scale Model for Coupling Strands with Shear-Dependent Liquid</b><br />
              ACM Transactions on Graphics (SIGGRAPH Asia), 2019, selected as the <a href="http://www.cs.columbia.edu/cg/creamystrand/2019-frontback.indd.pdf">front cover</a> <i class="fas fa-award"></i><br />
              <span class=name>
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://cs.uwaterloo.ca/~c2batty/">Christopher Batty</a>,
                            <a href="https://www.dgp.toronto.edu/~eitan/">Eitan Grinspun</a>,
                            <a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>
                            </span>
              <br />
              <a href="#abs-msmcssdl" class="open-popup-link">Abstract</a> | <a href="http://www.cs.columbia.edu/cg/creamystrand/">Project Page</a> | In Press: <a href="https://techxplore.com/news/2019-10-movement-simulating-complexity-fluids-strands.html">TechXplore</a>, <a href="https://www.eurekalert.org/pub_releases/2019-10/afcm-maf103119.php">AAAS EurekAlert!</a>, <a href="https://youtu.be/uVC5WowQxD8">Two Minute Papers</a><br />
              <span class="tag tag-blue">simulation</span>
            </p>
            <div id="abs-msmcssdl" class="white-popup mfp-hide">
              <p>
                We propose a framework for simulating the complex dynamics of strands interacting with compressible, shear-dependent liquids, such as oil paint, mud, cream, melted chocolate, and pasta sauce. Our framework contains three main components: the strands modeled as discrete rods, the bulk liquid represented as a continuum (material point method), and a reduced-dimensional flow of liquid on the surface of the strands with detailed elastoviscoplastic behavior. These three components are tightly coupled together. To enable discrete strands interacting with continuum-based liquid, we develop models that account for the volume change of the liquid as it passes through strands and the momentum exchange between the strands and the liquid. We also develop an extended constraint-based collision handling method that supports cohesion between strands. Furthermore, we present a principled method to preserve the total momentum of a strand and its surface flow, as well as an analytic plastic flow approach for Herschel-Bulkley fluid that enables stable semi-implicit integration at larger time steps. We explore a series of challenging scenarios, involving splashing, shaking, and agitating the liquid which causes the strands to stick together and become entangled.
              </p>
            </div>
            <P>
              <b>Addressing Troubles with Double Bubbles: Convergence and Stability at Multi-Bubble Junctions</b><br />
              Technical Report, 2019<br />
              <span class=name>
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://cs.uwaterloo.ca/~c2batty/">Christopher Batty</a>,
                            <a href="https://www.dgp.toronto.edu/~eitan/">Eitan Grinspun</a>
                            </span>
              <br />
              <a href="#abs-atdbc" class="open-popup-link">Abstract</a> | <a href="https://arxiv.org/abs/1910.06402">ArXiv Page</a> | <a href="https://github.com/nepluno/SoapFilm3D">GitHub</a><br />
              <span class="tag tag-blue">simulation</span>
            </p>
            <div id="abs-atdbc" class="white-popup mfp-hide">
              <p>
                In this report we discuss and propose a correction to a convergence and stability issue occurring in the work of Da et al.[2015], in which they proposed a numerical model to simulate soap bubbles.
              </p>
            </div>
            <P>
              <b>Mechanics-Aware Modeling of Cloth Appearance</b><br />
              IEEE Transactions on Visualization and Computer Graphics (Pacific Graphics), 2019<br />
              <span class=name>
                            <a href="https://www.research.manchester.ac.uk/portal/zahra.montazeri.html">Zahra Montazeri</a>,
                            <a href="http://chang.engineer">Chang Xiao</a>,
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>,
                            <a href="https://shuangz.com">Shuang Zhao</a>,
                            </span>
              <br />
              <a href="#abs-mamca" class="open-popup-link">Abstract</a> | <a href="https://arxiv.org/abs/1904.11116">Project Page</a> | <a href="https://www.computer.org/csdl/journal/tg/5555/01/08812922/1cPXcWXVlHq">IEEE Digital Library</a> | Video: <a href="https://youtu.be/10eD-tpFCNI" />YouTube</a>, <a href="https://www.bilibili.com/video/av50590036/">bilibili</a> | Talk at Pacific Graphics 2019: <a href="https://www.youtube.com/watch?v=JXYqCrdv1oA">YouTube</a><br />
              <span class="tag tag-blue">simulation</span> <span class="tag tag-blue2">rendering</span> <span class="tag tag-green2">machine learning</span>
            </p>
            <div id="abs-mamca" class="white-popup mfp-hide">
              <p>
                Micro-appearance models have brought unprecedented fidelity and details to cloth rendering. Yet, these models neglect fabric mechanics: when a piece of cloth interacts with the environment, its yarn and fiber arrangement usually changes in response to external contact and tension forces. Since subtle changes of a fabric's microstructures can greatly affect its macroscopic appearance, mechanics-driven appearance variation of fabrics has been a phenomenon that remains to be captured. We introduce a mechanics-aware model that adapts the microstructures of cloth yarns in a physics-based manner. Our technique works on two distinct physical scales: using physics-based simulations of individual yarns, we capture the rearrangement of yarn-level structures in response to external forces. These yarn structures are further enriched to obtain appearance-driving fiber-level details. The cross-scale enrichment is made practical through a new parameter fitting algorithm for simulation, an augmented procedural yarn model coupled with a custom-design regression neural network. We train the network using a dataset generated by joint simulations at both the yarn and the fiber levels. Through several examples, we demonstrate that our model is capable of synthesizing photorealistic cloth appearance in a %dynamic and mechanically plausible way.
              </p>
            </div>
            <P>
              <b>A Multi-Scale Model for Simulating Liquid-Fabric Interactions</b><br />
              ACM Transactions on Graphics (SIGGRAPH North America), 2018, appeared on the <a href="http://www.cs.columbia.edu/cg/wetcloth/frontmatter.pdf">back cover</a> <i class="fas fa-award"></i><br />
              <span class=name>
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="https://cs.uwaterloo.ca/~c2batty/">Christopher Batty</a>,
                            <a href="https://www.dgp.toronto.edu/~eitan/">Eitan Grinspun</a>,
                            <a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>
                            </span>
              <br />
              <a href="#abs-msmslf" class="open-popup-link">Abstract</a> | <a href="http://www.cs.columbia.edu/cg/wetcloth/">Project Page</a> | In Press: <a href="https://www.aps.org/apsnews/2025/03/lights-camera-physics-simulations">APS News</a>, <a href="https://www.deutschlandfunk.de/computergrafik-digitales-auswringen-von-nassen-handtuechern.684.de.html?dram:article_id=433457">Deutschlandfunk (in German)</a>, <a href="https://youtu.be/OEQf0AtSSsc">Two Minute Papers</a>, <a href="http://www.3dvf.com/actualite-23515-siggraph-2018-simulation-d-interactions-entre-tissu-et-fluides.html">3DVF (in French)</a>, <a href="http://www.cs.columbia.edu/2018/eight-papers-from-cs-researchers-are-part-of-siggraph-2018/">Columbia Engineering</a>, <a href="http://mp.163.com/v2/article/detail/DJ4GV1CO0516BJGJ.html">CGWorld (in Chinese)</a><br />
              <span class="tag tag-blue">simulation</span>
            </p>
            <div id="abs-msmslf" class="white-popup mfp-hide">
              <p>
                We propose a method for simulating the complex dynamics of partially and
                fully saturated woven and knit fabrics interacting with liquid, including
                the effects of buoyancy, nonlinear drag, pore (capillary) pressure,
                dripping, and convection-diffusion. Our model evolves the
                velocity fields of both the liquid and solid relying on mixture theory, as
                well as tracking a scalar saturation variable that affects the pore
                pressure forces in the fluid. We consider the porous microstructure implied
                by the fibers composing individual threads, and use it to derive homogenized drag and
                pore pressure models that faithfully reflect the anisotropy of fabrics. In
                addition to the bulk liquid and fabric motion, we derive a quasi-static
                flow model that accounts for liquid spreading within the
                fabric itself. Our implementation significantly extends standard numerical
                cloth and fluid models to support the diverse behaviors of wet fabric, and
                includes a numerical method tailored to cope with the challenging
                nonlinearities of the problem. We explore a range of fabric-water
                interactions to validate our model, including challenging animation
                scenarios involving splashing, wringing, and collisions with obstacles,
                along with qualitative comparisons against simple physical experiments.
              </p>
            </div>
            <P>
              <b>A Multi-Scale Model for Simulating Liquid-Hair Interactions</b><br>
              ACM Transactions on Graphics (SIGGRAPH North America), 2017<br />
              <span class=name>
                            <STRONG>Yun (Raymond) Fei</STRONG>,
                            <a href="http://www.columbia.edu/~htm2104/here/content/HenriqueMaia.pdf">Henrique Teles Maia</a>,
                            <a href="https://cs.uwaterloo.ca/~c2batty/">Christopher Batty</a>,
                            <a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>,
                            <a href="https://www.dgp.toronto.edu/~eitan/">Eitan Grinspun</a></span><br />
              <a href="#abs-msmsli" class="open-popup-link">Abstract</a> | <a href="http://www.cs.columbia.edu/cg/liquidhair/">Project Page</a> | In Press: <a href="https://80.lv/articles/a-multi-scale-model-for-simulating-liquid-hair-interactions/">80.lv</a>, <a href="https://youtu.be/ugdciqeOPeM">Two Minute Papers</a>, <a href="http://www.cgchannel.com/2017/06/videos-the-best-of-siggraph-2017s-technical-papers/">CG Channel</a>, <a href="https://www.pcgamer.com/the-coolest-tech-demos-from-siggraph-2017/">PC Gamer</a>, <a href="http://www.sohu.com/a/160499605_154576">CGWorld (in Chinese)</a>, <a href="http://3dnchu.com/archives/multi-scale-model-liquid-hair/">3Dnchu (in Japanese)</a>, <a href="https://mp.weixin.qq.com/s/ZnYBhbiC2DPziL1AjVQoMw">GeekiMovie (in Chinese)</a><br />
              <span class="tag tag-blue">simulation</span>
            </p>
            <div id="abs-msmsli" class="white-popup mfp-hide">
              <p>The diverse interactions between hair and liquid are complex and span multiple length scales, yet are central to the appearance of humans and animals in many situations. We therefore propose a novel multi-component simulation framework that treats many of the key physical mechanisms governing the dynamics of wet hair. The foundations of our approach are a discrete rod model for hair and a particle-in-cell model for fluids. To treat the thin layer of liquid that clings to the hair, we augment each hair strand with a height field representation. Our contribution is to develop the necessary physical and numerical models to evolve this new system and the interactions among its components. We develop a new reduced-dimensional liquid model to solve the motion of the liquid along the length of each hair, while accounting for its moving reference frame and influence on the hair dynamics. We derive a faithful model for surface tension-induced cohesion effects between adjacent hairs, based on the geometry of the liquid bridges that connect them. We adopt an empirically-validated drag model to treat the effects of coarse-scale interactions between hair and surrounding fluid, and propose new volume-conserving dripping and absorption strategies to transfer liquid between the reduced and particle-in-cell liquid representations. The synthesis of these techniques yields an effective wet hair simulator, which we use to animate hair flipping, an animal shaking itself dry, a spinning car wash roller brush dunked in liquid, and intricate hair coalescence effects, among several additional scenarios.</p>
            </div>
            <P>
              <b>Interactive Acoustic Transfer Approximation for Modal Sound</b><br />
              ACM Transactions on Graphics (SIGGRAPH North America), 2016<br />
              <span class=name>
                            <a href="http://dingzeyu.li">Dingzeyu Li</a>,
                            <STRONG>Yun Fei</STRONG>,
                            <a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a></span><br />
              <a href="#abs-iatams" class="open-popup-link">Abstract</a> | Preprint: <a href="http://www.cs.columbia.edu/cg/transfer/interactive-acoustic-transfer-approximation-for-modal-sound-tog-2016-compressed-li-et-al.pdf">Compressed</a>, <a href="http://www.cs.columbia.edu/cg/transfer/interactive-acoustic-transfer-approximation-for-modal-sound-tog-2016-li-et-al.pdf">Regular</a> | <a href="http://dl.acm.org/citation.cfm?id=2820612">ACM Digital Library</a> | Video: <a href="https://www.youtube.com/watch?v=tFKLo0yNmFk">YouTube</a>, <a href="http://www.bilibili.com/video/av4489052/">bilibili</a> | <a href="https://www.youtube.com/watch?v=MxkPRKItwyo">Fast Forward</a> | <a href="http://www.cs.columbia.edu/cg/transfer/">Project Page</a><br />
              <span class="tag tag-blue">simulation</span>
            </p>
            <div id="abs-iatams" class="white-popup mfp-hide">
              <p>Current linear modal sound models are tightly coupled with their frequency content. Both the modal vibration of object surfaces and the resulting sound radiation depend on the vibration frequency. Whenever the user tweaks modal parameters to adjust frequencies the modal sound model changes completely, necessitating expensive recomputation of modal vibration and sound radiation.</p>
              <p>We propose a new method for interactive and continuous editing as well as exploration of modal sound parameters. We start by sampling a number of key points around a vibrating object, and then devise a compact, low-memory representation of frequency-varying acoustic transfer values at each key point using Prony series. We efficiently precompute these series using an adaptive frequency sweeping algorithm and volume-velocity-preserving mesh simplification. At runtime, we approximate acoustic transfer values using standard multipole expansions. Given user-specified modal frequencies, we solve a small least-squares system to estimate the expansion coefficients, and thereby quickly compute the resulting sound pressure value at arbitrary listening locations. We demonstrate the numerical accuracy, the runtime performance of our method on a set of comparisons and examples, and evaluate sound quality with user perception studies.</p>
            </div>
            <P>
              <b>Computational Design of Metallophone Contact Sounds</b><br />
              ACM Transactions on Graphics (SIGGRAPH Asia), 2015<br />
              <span class=name>
                            <a href="https://gauravbharaj.github.io/">Gaurav Bharaj</a>,
                            <a href="http://diwlevin.com/">David I. W. Levin</a>,
                            <a href="http://www.jamestompkin.com">James Tompkin</a>,
                            <STRONG>Yun Fei</STRONG>,
                            <a href="http://vcg.seas.harvard.edu">Hanspeter Pfister</a>,
                            <a href="http://people.csail.mit.edu/wojciech/">Wojciech Matusik</a>,
                            <a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a></span><br />
              <a href="#abs-cdmcs" class="open-popup-link">Abstract</a> | <a href="http://people.seas.harvard.edu/~gaurav/papers/cdmcs_sa_2015/cdmcs_sa_2015.pdf">Preprint</a> | <a href="http://dl.acm.org/citation.cfm?id=2818108">ACM Digital Library</a> | <a href="http://people.seas.harvard.edu/~gaurav/papers/cdmcs_sa_2015/cdwa_supp_sca_2015.pdf">Supplementary</a> | Video: <a href="https://www.youtube.com/watch?v=Ij2x1kT6aLQ">YouTube</a>, <a href="http://www.bilibili.com/video/av4494339/">bilibili</a> | <a href="http://people.seas.harvard.edu/~gaurav/papers/cdmcs_sa_2015/">Project page</a> | <a href="http://www.bigbluesaw.com/big-blue-saw-blog/general-updates/the-zoolophone-reloaded.html">Industrial Reproduction</a> | In Press: <a href="http://science360.gov/obj/video/35aecb16-96d4-4f2d-b817-265595f46669/extra-6-animal-sounds">Science 360</a>, <a href="http://www.wired.com/2015/11/this-animal-shaped-glockenspiel-is-really-a-rad-experiment">Wired</a>, <a href="http://www.popsci.com/3-d-printed-zoolophone-demonstrates-connection-between-shape-and-sound">Popular Science</a>, <a href="http://gizmodo.com/you-need-geniuses-at-mit-harvard-and-columbia-to-make-1739689295">Gizmodo</a>, <a href="https://www.inverse.com/article/7556-a-computer-scientist-is-building-the-musical-instruments-of-the-future">Inverse</a>, <a href="http://engineering.columbia.edu/change-shape-change-sound">Columbia Engineering</a>, <a href="http://www.sciencedaily.com/releases/2015/10/151029103401.htm">Science Daily</a>, <a href="http://www.gizmag.com/zoolophone/40175/">Gizmag</a>, <a href="http://www.techradar.com/us/news/world-of-tech/would-you-like-to-play-this-3d-printed-zoolophone--1307740">TechRadar</a>, <a href="http://3dfabprint.com/zoolophone-musical-instrument-made-of-animal-shapes/">3DFab+Print</a><br />
              <span class="tag tag-blue">simulation</span> <span class="tag tag-green">fabrication</span>
            </p>
            <div id="abs-cdmcs" class="white-popup mfp-hide">
              <p>Metallophones such as glockenspiels produce sounds in response to contact. Building these instruments is a complicated process, limiting their shapes to well-understood designs such as bars. We automatically optimize the shape of arbitrary 2D and 3D objects through deformation and perforation to produce sounds when struck which match user-supplied frequency and amplitude spectra. This optimization requires navigating a complex energy landscape, for which we develop Latin Complement Sampling to both speed up finding minima and provide probabilistic bounds on landscape exploration. Our method produces instruments which perform similarly to those that have been professionally-manufactured, while also expanding the scope of shape and sound that can be realized, e.g., single object chords. Furthermore, we can optimize sound spectra to create overtones and to dampen specific frequencies. Thus our technique allows even novices to design metallophones with unique sound and appearance.</p>
            </div>
            <P><b>On the Hessian of Shape Matching Energy</b><br />
              Technical Report, 2016<br />
              <STRONG>Yun Fei</STRONG><br />
              <a href="http://arxiv.org/abs/1604.02483">ArXiv Page</a><br />
              <span class="tag tag-blue">simulation</span>
            </P>
            <P>
              <b>SmartGuide: Towards Single-image Building Localization with Smartphone</b><br />
              Proceedings of MobiHoc 2015<br />
              <span class=name>
                            <a href="https://www.linkedin.com/in/xixiong91/">Xi Xiong</a>,
                            <a href="http://tns.thss.tsinghua.edu.cn/~yangzheng/">Yang Zheng</a>,
                            <a href="https://shanggdlk.github.io/">Longfei Shangguan</a>,
                            <STRONG>Yun Fei</STRONG>,
                            <a href="http://milos.stojmenovic.com/Home.html">Miloš Stojmenović</a>,
                            <a href="http://www.cse.msu.edu/~liuyunha/">Yunhao Liu</a></span><br />
              <a href="https://dl.acm.org/doi/10.1145/2746285.2746294">ACM Digital Library</a><br />
              <span class="tag tag-cyan">mobile computing</span>
            </P>
            <P>
              <b>GPU Parallelization of SPH and Its Applications</b><br />
              Dissertation at Tsinghua University, 2013<br />
              <span class=name>
                            Advisors: <a href="https://binwangthss.github.io/">Bin Wang</a>
                            </span>
              <br />
              <a href="#abs-gpsia" class="open-popup-link">Abstract</a> | <a href="thesis/Bach_Dissertation.pdf">Preprint (in Chinese)</a><br />
              <span class="tag tag-blue">simulation</span> <span class="tag tag-purple">GPU</span> <span class="tag tag-blue2">rendering</span>
            </P>
            <div id="abs-gpsia" class="white-popup mfp-hide">
              <p>Recently, the power of graphics processing unit (GPU) has been incredibly improved
                due to the development of hardware technology, in programming shading and in general
                purpose computing, which is much higher than traditional central processing units (CPU).
                Hence, in this thesis, we apply it to fluid simulation and rendering, which includes two main contributions: <br />
                1. a parallelized implementation of predictive-corrective incompressible smoothed
                particle hydrodynamics, which is a recently popular method for the simulation of
                incompressible fluid, producing real-time frame rate.<br />
                2. a parallelized implementation of smoothed surface reconstruction from 3D point
                cloud, for real-time rendering of the fluids motion.<br />
                Furthermore, the implementation accompanied with this thesis is the first time that
                the simulation of incompressible fluids, surface reconstruction and rendering are
                integrated into an open source package, which can be an effective start for further
                research and applications. To be concrete, we demonstrate two application of our fluid solver:<br />
                1. by introducing geometry-based caustics and shadows, we plausibly simulate and render
                the fluid in real-time.<br />
                2. by introducing temperate-related viscosity, and fluid-solid coupling, we simu-late the
                painting process of Chinese sugar-coating in an interactive way.
              </p>
            </div>
            <P>
              <b>Parallel L-BFGS-B algorithm on GPU</b><br />
              Computers and Graphics, 2014<br />
              <span class=name>
                            <STRONG>Yun Fei</STRONG>, <a href="https://sites.google.com/site/rongguodong/">Guodong Rong</a>, <a href="https://binwangthss.github.io/">Bin Wang</a>, <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a></span><br />
              <a href="#abs-plag" class="open-popup-link">Abstract</a> | <a href="http://dx.doi.org/10.1016/j.cag.2014.01.002">ScienceDirect</a> | <a href="lbfgsb/lbfgsb_tech_report.pdf">Preprint</a> | <a href="https://github.com/nepluno/lbfgsb-gpu">Github</a><br />
              <span class="tag tag-purple">GPU</span> <span class="tag tag-orange">optimization</span>
            </p>
            <div id="abs-plag" class="white-popup mfp-hide">
              <p>Due to the rapid advance of general-purpose graphics processing unit (GPU), it is an active research topic to study performance improvement of non-linear optimization with parallel implementation on GPU, as attested by the much research on parallel implementation of relatively simple optimization methods, such as the conjugate gradient method. We study in this context the L-BFGS-B method, or the limited memory Broyden–Fletcher–Goldfarb–Shanno with boundaries, which is a sophisticated yet efficient optimization method widely used in computer graphics as well as general scientific computation. By analyzing and resolving the inherent dependencies of some of its search steps, we propose an efficient GPU-based parallel implementation of L-BFGS-B on the GPU. We justify our design decisions and demonstrate significant speed-up by our parallel implementation in solving the centroidal Voronoi tessellation (CVT) problem as well as some typical computing problems.</p>
            </div>
            <P>
              <b>Towards Photo Watercolorization with Artistic Verisimilitude</b><br />
              IEEE Transactions on Visualization and Computer Graphics, 2014<br />
              <span class=name>
                            <a href="https://www.linkedin.com/in/wangmiaoyi/">Miaoyi Wang</a>,
                            <a href="https://binwangthss.github.io/">Bin Wang</a>,
                            <STRONG>Yun Fei</STRONG>,
                            <a href="http://qiankanglai.me/">Kang-Lai Qian</a>,
                            <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>,
                            <a href="https://www.linkedin.com/in/%E5%AE%B6%E6%8C%BA-%E9%99%88-3a90952b/">Jiating Chen</a>,
                            <a href="https://scholar.google.com/citations?user=pBCA92wAAAAJ&hl=zh-CN">Jun-Hai Yong</a></span><br />
              <a href="#abs-tpwav" class="open-popup-link">Abstract</a> | <a href="watercolor/watercolor_pp.pdf">Preprint</a> | <a href="http://dx.doi.org/10.1109/TVCG.2014.2303984">IEEE Xplore</a><br />
              <span class="tag tag-red2">image</span> <span class="tag tag-blue2">rendering</span>
            </p>
            <div id="abs-tpwav" class="white-popup mfp-hide">
              <p>We present a novel artistic-verisimilitude driven system for watercolor rendering of images and photos. Our system achieves realistic simulation of a set of important characteristics of watercolor paintings that have not been well implemented before. Specifically, we designed several image filters to achieve: 1) watercolor-specified color transferring; 2) saliency-based level-of-detail drawing; 3) hand tremor effect due to human neural noise; and 4) an artistically controlled wet-in-wet effect in the border regions of different wet pigments. A user study indicates that our method can produce watercolor results of artistic verisimilitude better than previous filter-based or physical-based methods. Furthermore, our algorithm is efficient and can easily be parallelized, making it suitable for interactive image watercolorization.</p>
            </div>
            <P>
              <b>Bilateral Blue Noise Sampling</b><br />
              ACM Transactions on Graphics (SIGGRAPH Asia), 2013, selected as the <a href="https://dl.acm.org/action/showFmPdf?doi=10.1145%2F2508363">front cover</a> <i class="fas fa-award"></i><br />
              <span class=name>
                            <a href="https://www.linkedin.com/in/%E5%AE%B6%E6%8C%BA-%E9%99%88-3a90952b/">Jiating Chen</a>*,
                            <a href="https://www.linkedin.com/in/xiaoyin-ge-75614516">Xiaoyin Ge</a>*,
                            <a href="http://www.liyiwei.org/">Li-Yi Wei</a>,
                            <a href="https://binwangthss.github.io/">Bin Wang</a>,
                            <a href="https://web.cse.ohio-state.edu/~wang.1016/">Yusu Wang</a>,
                            <a href="http://www.cse.ohio-state.edu/~whmin/">Huamin Wang</a>,
                            <STRONG>Yun Fei</STRONG>,
                            <a href="http://qiankanglai.me/">Kang-Lai Qian</a>,
                            <a href="https://scholar.google.com/citations?user=pBCA92wAAAAJ&hl=zh-CN">Jun-Hai Yong</a>,
                            <a href="http://i.cs.hku.hk/~wenping/">Wenping Wang</a>
                            (*Joint 1st authors)</span><br />
              <a href="#abs-bbns" class="open-popup-link">Abstract</a> | <a href="bluenoise/final_opt.pdf">Preprint (Compressed)</a> |
              Video: <a href="http://www.youtube.com/watch?v=mJOvenBySGs">YouTube</a>, <a href="http://www.bilibili.com/video/av4508623/">bilibili</a> | <a href="https://www.youtube.com/watch?v=Df3UtLAwQBs">Image Stippling</a>
              | <a href="https://github.com/nepluno/bilateralbluenoise">GitHub</a> | <a href="http://www.liyiwei.org/papers/noise-siga13/">Project Page</a><br />
              <span class="tag tag-orange">geometry</span> <span class="tag tag-blue2">rendering</span>
            </P>
            <div id="abs-bbns" class="white-popup mfp-hide">
              <p>Blue noise sampling is an important component in many graphics applications, but existing techniques consider mainly the spatial positions of samples, making them less effective when handling problems with non-spatial features. Examples include biological distribution in which plant spacing is influenced by non-positional factors such as tree type and size, photon mapping in which photon flux and direction are not a direct function of the attached surface, and point cloud sampling in which the underlying surface is unknown a priori. These scenarios can benefit from blue noise sample distributions, but cannot be adequately handled by prior art.</p>
              <p>Inspired by bilateral filtering, we propose a bilateral blue noise sampling strategy. Our key idea is a general formulation to modulate the traditional sample distance measures, which are determined by sample position in spatial domain, with a similarity measure that considers arbitrary per sample attributes. This modulation leads to the notion of bilateral blue noise whose properties are influenced by not only the uniformity of the sample positions but also the similarity of the sample attributes. We describe how to incorporate our modulation into various sample analysis and synthesis methods, and demonstrate applications in object distribution, photon density estimation, and point cloud sub-sampling.</p>
            </div>
            <P>
              <b>Point-Tessellated Voxelization</b><br />
              Proceedings of Graphics Interface (GI), 2012<br />
              <span class=name>
                            <STRONG>Yun Fei</STRONG>, <a href="https://binwangthss.github.io/">Bin Wang</a>, and <a href="https://www.linkedin.com/in/%E5%AE%B6%E6%8C%BA-%E9%99%88-3a90952b/">Jiating Chen</a></span><br />
              <a href="#abs-ptv" class="open-popup-link">Abstract</a> | <a href="http://dl.acm.org/citation.cfm?id=2305280">ACM digital library</a> | <a href="gi12/paper100.pdf">Preprint</a> | Video: <a href="https://www.youtube.com/watch?v=_9KLhs2dtOY">YouTube</a>, <a href="http://www.bilibili.com/video/av4517258/">bilibili</a><br />
              <span class="tag tag-purple">GPU</span> <span class="tag tag-orange">geometry</span> <span class="tag tag-blue2">rendering</span>
            </P>
            <div id="abs-ptv" class="white-popup mfp-hide">
              <p>Applications such as shape matching, visibility processing, rapid manufacturing, and 360 degree display usually require the generation of a voxel representation from a triangle mesh interactively or in real-time. In this paper, we describe a novel framework that uses the hardware tessellation support on the graphics processing unit (GPU) for surface voxelization. To generate gap-free voxelization results with superior performance, our framework uses three stages: triangle subdivision, point generation, and point injection. For even higher temporal efficiency we introduce PN-triangles and displacement mapping to voxelize meshes with rugged surfaces in high resolution.</p>
              <p>Our framework can be implemented with simple shader programming, making it readily applicable to a number of real-time applications where both development and runtime efficiencies are of concern.</p>
            </div>
            <P>
              <b>Fast multi-image-based photon tracing with grid-based gathering</b><br />
              SIGGRAPH Poster, 2012<br />
              <span class=name>
                            <STRONG>Yun Fei</STRONG>,
                            <a href="https://binwangthss.github.io/">Bin Wang</a></span><br />
              <a href="sig12poster/poster.pdf">Poster</a> | <a href="https://www.youtube.com/watch?v=fC6vGrRjF_Y">YouTube</a><br />
              <span class="tag tag-purple">GPU</span> <span class="tag tag-blue2">rendering</span>
            </P>
          </DIV>
        </DIV>
        <DIV class=x></DIV>
        <!-- footer -->
        <DIV id=footer>
          ©2012-2023. All Rights Reserved.
        </DIV>
      </DIV>
    </DIV>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js'></script>
    <script src="dist/lovely-tag.js"></script>
    <script>
      $('.open-popup-link').magnificPopup({
        type: 'inline',
        midClick: true
      });
    </script>
  </BODY>
</HTML>
